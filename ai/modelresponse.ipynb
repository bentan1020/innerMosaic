from google.colab import drive
import pandas as pd
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional
from tensorflow.keras.models import Sequential
from sklearn.model_selection import train_test_split
import tensorflow as tf
import numpy as np

drive.mount("/content/drive")

dataset_csv_path = "/content/drive/My Drive/outputdata - outputdata.csv"

data = pd.read_csv(dataset_csv_path)

input_sequences = data["Answer"].astype(str).tolist()
output_sequences = data["reflective_response"].astype(str).tolist()

tokenizer = Tokenizer()
tokenizer.fit_on_texts(input_sequences + output_sequences)
total_words = len(tokenizer.word_index) + 1

input_sequences = tokenizer.texts_to_sequences(input_sequences)
output_sequences = tokenizer.texts_to_sequences(output_sequences)

max_len = max(max(len(x) for x in input_sequences), max(len(x) for x in output_sequences))
input_padded = pad_sequences(input_sequences, maxlen=max_len, padding="post")
output_padded = pad_sequences(output_sequences, maxlen=max_len, padding="post")

input_train, input_val, output_train, output_val = train_test_split(input_padded, output_padded, test_size=0.2)

print("Shape of output_train:", output_train.shape)

model = Sequential([
    Embedding(total_words, 64, input_length=max_len),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dense(total_words, activation="softmax")
])

model.compile(loss="sparse_categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

try:
    model.fit(input_train, output_train, epochs=10, validation_data=(input_val, output_val))
except Exception as e:
    print("Error during model fit:", e)
    print("Input train shape:", input_train.shape)
    print("Output train shape:", output_train.shape)
    print("Input validation shape:", input_val.shape)
    print("Output validation shape:", output_val.shape)
    raise

def generate_reflective_response(model, tokenizer, seed_text, max_gen_length):
    input_text = seed_text
    generated_sequence = []

    for _ in range(max_gen_length):
        input_sequence = tokenizer.texts_to_sequences([input_text])[0]


        input_sequence = np.array(input_sequence)[np.newaxis, :]

        predicted_probs = model.predict(input_sequence)[0]


        predicted_index = tf.random.categorical(predicted_probs, num_samples=1)[-1,0].numpy()


        if predicted_index == 0:
            break


        predicted_word = tokenizer.index_word.get(predicted_index, '')


        generated_sequence.append(predicted_word)

        input_text += ' ' + predicted_word

    generated_response = ' '.join(generated_sequence)

    return generated_response


sample_entry = "Today I cooked a nice meal with my family."
generated_response = generate_reflective_response(model, tokenizer, sample_entry, max_len)
print(generated_response)
